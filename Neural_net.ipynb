{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../../../../Downloads/human_matrix.h5', 'r')\n",
    "expression = f['data']['expression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5000\n",
    "num_genes = 500\n",
    "expression_small = expression[np.sort(np.random.choice(expression.shape[0],num_samples,replace=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_small = expression_small[:,np.sort(np.random.choice(expression_small.shape[1],num_genes,replace=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulated measurements\n",
    "measurements = 300\n",
    "A = np.random.normal(size=(measurements,num_genes))\n",
    "compressed_mes = np.dot(A,expression_small.T).T\n",
    "X_train, X_test,y_train, y_test = train_test_split(compressed_mes,expression_small,test_size=0.2)\n",
    "X_train_std = np.std(X_train,axis=0)\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "X_train = np.divide(X_train - X_train_mean,X_train_std)\n",
    "X_test = np.divide(X_test - X_train_mean,X_train_std)\n",
    "\n",
    "# perform standardization on labels\n",
    "y_train_std = np.std(y_train,axis=0)\n",
    "y_train_mean = np.mean(y_train,axis=0)\n",
    "y_stand = np.divide(y_train - y_train_mean,y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=500, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(measurements,200)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(200, 300)\n",
    "#         self.fc4 = nn.Linear(300, 300) #added\n",
    "        self.fc3 = nn.Linear(300, num_genes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "#         x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "#         # If the size is a square you can only specify a single number\n",
    "#         x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc4(x)) #added\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net = net.double()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([200, 300])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(float))\n",
    "X_test = torch.from_numpy(X_test.astype(float))\n",
    "y_train = torch.from_numpy(y_train.astype(float))\n",
    "y_test = torch.from_numpy(y_test.astype(float))\n",
    "y_stand = torch.from_numpy(y_stand.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000, 300])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.00000000000001)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0853, 0.0000, 0.1209, 0.0000, 0.0238, 0.0000, 0.0000, 0.0000, 0.0289,\n",
      "        0.0000, 0.0317, 0.0035, 0.0000, 0.0000, 0.0000, 0.0000, 0.0691, 0.0748,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0034, 0.0094, 0.0279, 0.0663, 0.0749,\n",
      "        0.0000, 0.0662, 0.0906, 0.0356, 0.1655, 0.0000, 0.1049, 0.0455, 0.1046,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1807, 0.1028,\n",
      "        0.0000, 0.0716, 0.0000, 0.0263, 0.0000, 0.0311, 0.0000, 0.0166, 0.0177,\n",
      "        0.0000, 0.0000, 0.0000, 0.0088, 0.0000, 0.0140, 0.0000, 0.1871, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0124, 0.0000, 0.0787, 0.1091, 0.0000, 0.1127,\n",
      "        0.0886, 0.0000, 0.0536, 0.0007, 0.0000, 0.0630, 0.0459, 0.0000, 0.0822,\n",
      "        0.0953, 0.0000, 0.0000, 0.0000, 0.0689, 0.0203, 0.0442, 0.0060, 0.1770,\n",
      "        0.0000, 0.0321, 0.0514, 0.0838, 0.0000, 0.0000, 0.0000, 0.0782, 0.0893,\n",
      "        0.0456, 0.1656, 0.0000, 0.0000, 0.1241, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0739, 0.0000, 0.0000, 0.0000, 0.0203, 0.0000, 0.0000,\n",
      "        0.1931, 0.0000, 0.1344, 0.0000, 0.0525, 0.0605, 0.1902, 0.0000, 0.0982,\n",
      "        0.0000, 0.0754, 0.0943, 0.0848, 0.0000, 0.0000, 0.0000, 0.2199, 0.0036,\n",
      "        0.0000, 0.0000, 0.1530, 0.0304, 0.0000, 0.0000, 0.0453, 0.0000, 0.1314,\n",
      "        0.0000, 0.1325, 0.0000, 0.0722, 0.0000, 0.0137, 0.0710, 0.0000, 0.0000,\n",
      "        0.0083, 0.1098, 0.0252, 0.0000, 0.1822, 0.0014, 0.0000, 0.0877, 0.0000,\n",
      "        0.0000, 0.1241, 0.0342, 0.0000, 0.0000, 0.0753, 0.0114, 0.0000, 0.0000,\n",
      "        0.0642, 0.0000, 0.0000, 0.0875, 0.0000, 0.1174, 0.0140, 0.0994, 0.0684,\n",
      "        0.1956, 0.0136, 0.0114, 0.1640, 0.0000, 0.0000, 0.0000, 0.2055, 0.0000,\n",
      "        0.0143, 0.0000, 0.0870, 0.0542, 0.0559, 0.0000, 0.0000, 0.0000, 0.0501,\n",
      "        0.0000, 0.0231, 0.0000, 0.0000, 0.1207, 0.0000, 0.0269, 0.0436, 0.0927,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0256, 0.0000, 0.1392, 0.0000, 0.0625,\n",
      "        0.0396, 0.0000, 0.0812, 0.0000, 0.0000, 0.0000, 0.0045, 0.1541, 0.0000,\n",
      "        0.1952, 0.0117, 0.0000, 0.0000, 0.1110, 0.0306, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.1054, 0.0348, 0.0000, 0.0094, 0.0737,\n",
      "        0.0038, 0.0000, 0.0810, 0.0000, 0.0000, 0.0000, 0.0000, 0.1022, 0.0000,\n",
      "        0.1226, 0.0558, 0.0000, 0.0225, 0.0000, 0.0000, 0.0000, 0.2404, 0.0000,\n",
      "        0.0045, 0.0000, 0.0814, 0.0000, 0.0191, 0.0000, 0.0275, 0.0000, 0.0000,\n",
      "        0.1079, 0.0000, 0.0967, 0.0000, 0.1527, 0.0000, 0.0000, 0.0159, 0.0000,\n",
      "        0.0534, 0.0000, 0.0000, 0.0602, 0.0000, 0.0181, 0.0000, 0.0000, 0.1940,\n",
      "        0.0138, 0.0160, 0.0000, 0.0000, 0.0524, 0.0171, 0.0518, 0.0432, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0641, 0.0000, 0.0858, 0.0000,\n",
      "        0.0000, 0.0479, 0.1823, 0.0000, 0.0000, 0.0000, 0.0502, 0.0287, 0.0000,\n",
      "        0.0166, 0.0000, 0.0000, 0.0000, 0.0892, 0.0000, 0.0000, 0.0061, 0.0000,\n",
      "        0.0304, 0.2155, 0.0000, 0.0471, 0.0655, 0.0084, 0.0383, 0.0324, 0.0606,\n",
      "        0.0000, 0.0000, 0.0000, 0.0571, 0.0000, 0.0000, 0.0268, 0.0741, 0.0233,\n",
      "        0.0000, 0.0665, 0.0000, 0.0893, 0.0148, 0.0000, 0.1123, 0.1977, 0.2416,\n",
      "        0.0873, 0.0000, 0.0688, 0.0297, 0.0000, 0.0473, 0.0011, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0561, 0.0568, 0.1192, 0.0000, 0.0281, 0.0000,\n",
      "        0.0736, 0.0000, 0.0000, 0.0000, 0.0895, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.1455, 0.0000, 0.0000, 0.0000, 0.0000, 0.1522, 0.1626, 0.0334,\n",
      "        0.0000, 0.1208, 0.1165, 0.0000, 0.0000, 0.1025, 0.0947, 0.1375, 0.0703,\n",
      "        0.0000, 0.0047, 0.0836, 0.0000, 0.0000, 0.0000, 0.1273, 0.0000, 0.0900,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0721, 0.0503, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0756, 0.0000, 0.0000, 0.0466, 0.0000, 0.0185, 0.1546, 0.0000,\n",
      "        0.0000, 0.0000, 0.1083, 0.0000, 0.1143, 0.0000, 0.1491, 0.0000, 0.0153,\n",
      "        0.0159, 0.0000, 0.0008, 0.0000, 0.0000, 0.0932, 0.0000, 0.0000, 0.0000,\n",
      "        0.2114, 0.0079, 0.0000, 0.0262, 0.0000, 0.0000, 0.0338, 0.0000, 0.1025,\n",
      "        0.1231, 0.0589, 0.0000, 0.0000, 0.1505, 0.0539, 0.0000, 0.1867, 0.0000,\n",
      "        0.0000, 0.0183, 0.0000, 0.0000, 0.0000, 0.1535, 0.0000, 0.0605, 0.0320,\n",
      "        0.0258, 0.0809, 0.0629, 0.0000, 0.1455, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0556, 0.0000, 0.0318, 0.0045, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1306, 0.0037, 0.0142,\n",
      "        0.0282, 0.0951, 0.0334, 0.0000, 0.0217], dtype=torch.float64,\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out= net(X_train[0,:])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3993.6661970086984\n",
      "1\n",
      "3990.8121475330186\n",
      "2\n",
      "3988.1542405058353\n",
      "3\n",
      "3985.613976722891\n",
      "4\n",
      "3983.1474752377558\n",
      "5\n",
      "3980.7418489675\n",
      "6\n",
      "3978.3730525280944\n",
      "7\n",
      "3976.0271033336903\n",
      "8\n",
      "3973.697810457038\n",
      "9\n",
      "3971.3567553657335\n",
      "10\n",
      "3968.999427208863\n",
      "11\n",
      "3966.6144881410937\n",
      "12\n",
      "3964.17048411419\n",
      "13\n",
      "3961.685406517444\n",
      "14\n",
      "3959.137182174774\n",
      "15\n",
      "3956.5164903392565\n",
      "16\n",
      "3953.831289018844\n",
      "17\n",
      "3951.065835174636\n",
      "18\n",
      "3948.2211280080346\n",
      "19\n",
      "3945.2978752908593\n",
      "20\n",
      "3942.2911826272116\n",
      "21\n",
      "3939.1836294980076\n",
      "22\n",
      "3935.9896486350226\n",
      "23\n",
      "3932.7234565630747\n",
      "24\n",
      "3929.3767819954496\n",
      "25\n",
      "3925.957274490393\n",
      "26\n",
      "3922.464065958507\n",
      "27\n",
      "3918.9113685347115\n",
      "28\n",
      "3915.2845922220627\n",
      "29\n",
      "3911.5979559769194\n",
      "30\n",
      "3907.8788071763597\n",
      "31\n",
      "3904.1212234351883\n",
      "32\n",
      "3900.320702328775\n",
      "33\n",
      "3896.494790606937\n",
      "34\n",
      "3892.627959607259\n",
      "35\n",
      "3888.7417047791487\n",
      "36\n",
      "3884.8559519257365\n",
      "37\n",
      "3880.9501895121175\n",
      "38\n",
      "3877.0465851011995\n",
      "39\n",
      "3873.1639069689827\n",
      "40\n",
      "3869.299128536111\n",
      "41\n",
      "3865.4509287687\n",
      "42\n",
      "3861.6318732285717\n",
      "43\n",
      "3857.8532524590646\n",
      "44\n",
      "3854.076477308363\n",
      "45\n",
      "3850.3230823882805\n",
      "46\n",
      "3846.623206717093\n",
      "47\n",
      "3842.9166489265185\n",
      "48\n",
      "3839.229823391148\n",
      "49\n",
      "3835.581957840461\n",
      "50\n",
      "3831.9670629965644\n",
      "51\n",
      "3828.382580457949\n",
      "52\n",
      "3824.8316506920887\n",
      "53\n",
      "3821.3571475088233\n",
      "54\n",
      "3817.9366980609984\n",
      "55\n",
      "3814.619058582595\n",
      "56\n",
      "3811.3842442769615\n",
      "57\n",
      "3808.241399403237\n",
      "58\n",
      "3805.190365092801\n",
      "59\n",
      "3802.207574780285\n",
      "60\n",
      "3799.3185518005193\n",
      "61\n",
      "3796.495819852996\n",
      "62\n",
      "3793.74514829821\n",
      "63\n",
      "3791.0573595778164\n",
      "64\n",
      "3788.4268286413812\n",
      "65\n",
      "3785.8590521584224\n",
      "66\n",
      "3783.3315186262366\n",
      "67\n",
      "3780.8444072051852\n",
      "68\n",
      "3778.3899444775316\n",
      "69\n",
      "3775.9701035375765\n",
      "70\n",
      "3773.552507856598\n",
      "71\n",
      "3771.087418644442\n",
      "72\n",
      "3768.564672055139\n",
      "73\n",
      "3766.006760683509\n",
      "74\n",
      "3763.413304308792\n",
      "75\n",
      "3760.7636724286567\n",
      "76\n",
      "3758.13166158668\n",
      "77\n",
      "3755.52919419502\n",
      "78\n",
      "3752.959723227136\n",
      "79\n",
      "3750.4236011460366\n",
      "80\n",
      "3747.9060235122583\n",
      "81\n",
      "3745.401457691119\n",
      "82\n",
      "3742.9006273678465\n",
      "83\n",
      "3740.400127908907\n",
      "84\n",
      "3737.9037804618715\n",
      "85\n",
      "3735.3881793341197\n",
      "86\n",
      "3732.850911340469\n",
      "87\n",
      "3730.289506327124\n",
      "88\n",
      "3727.7104631255734\n",
      "89\n",
      "3725.1174362605457\n",
      "90\n",
      "3722.515653784826\n",
      "91\n",
      "3719.8784199651554\n",
      "92\n",
      "3717.251412132771\n",
      "93\n",
      "3714.648377348537\n",
      "94\n",
      "3712.06382362218\n",
      "95\n",
      "3709.535564427022\n",
      "96\n",
      "3707.0609585792563\n",
      "97\n",
      "3704.6327297101334\n",
      "98\n",
      "3702.258610042616\n",
      "99\n",
      "3699.932436007458\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    print(epoch)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(X_train, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data\n",
    "        labels = y_stand[i,:]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "#         print(inputs)\n",
    "        outputs = net(inputs)\n",
    "#         print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "#         if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 100))\n",
    "#             running_loss = 0.0\n",
    "    print(running_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6443675352270476\n"
     ]
    }
   ],
   "source": [
    "X_test_hat = np.zeros((y_test.shape[0],y_test.shape[1]))\n",
    "total_correlation = 0\n",
    "for i, data in enumerate(X_test,0):\n",
    "    out = net(data)\n",
    "#     pearson_corr = scipy.stats.pearsonr(out.data.numpy(),y_test[i,:].data.numpy())\n",
    "    out = np.multiply(out.data.numpy()+y_train_mean,y_train_std)\n",
    "    pearson_corr = scipy.stats.pearsonr(out,y_test[i,:].data.numpy())\n",
    "    if (math.isnan(pearson_corr[0])):\n",
    "        total_correlation = total_correlation+0\n",
    "    else:\n",
    "        total_correlation = pearson_corr[0] + total_correlation\n",
    "print(total_correlation/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6465869838090701\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "total=0.0\n",
    "count = 0\n",
    "for i, data in enumerate(X_train,0):\n",
    "    out = net(data)\n",
    "#     pearson_corr = scipy.stats.pearsonr(out.data.numpy(),y_train[i,:].data.numpy())\n",
    "    out = np.multiply(out.data.numpy()+y_train_mean,y_train_std)\n",
    "    pearson_corr = scipy.stats.pearsonr(out,y_train[i,:].data.numpy())\n",
    "    if (math.isnan(pearson_corr[0])):\n",
    "        total = total+0\n",
    "    else:\n",
    "        total = pearson_corr[0] + total\n",
    "        count = count + 1\n",
    "#     print(total)\n",
    "print(total/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running neural net with sci-kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(300, 300, 300, 300), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(300, 300,300, 300), max_iter=1000)\n",
    "mlp.fit(X_train.data.numpy(), y_train.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mlp.predict(X_test.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7154320872341129\n"
     ]
    }
   ],
   "source": [
    "total_correlation = 0\n",
    "for i in range(0,out.shape[0]):\n",
    "    pearson_corr = scipy.stats.pearsonr(out[i,:],y_test[i,:].data.numpy())\n",
    "    if (math.isnan(pearson_corr[0])):\n",
    "        total_correlation = total_correlation+0\n",
    "    else:\n",
    "        total_correlation = pearson_corr[0] + total_correlation\n",
    "print(total_correlation/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.707435239646608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "out2 = mlp.predict(X_train.data.numpy())\n",
    "total_correlation = 0\n",
    "for i in range(0,out2.shape[0]):\n",
    "    pearson_corr = scipy.stats.pearsonr(out2[i,:],y_train[i,:].data.numpy())\n",
    "    if (math.isnan(pearson_corr[0])):\n",
    "        total_correlation = total_correlation+0\n",
    "    else:\n",
    "        total_correlation = pearson_corr[0] + total_correlation\n",
    "print(total_correlation/y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 0.0)"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.pearsonr([1.0,1.0,2.0],[3.0,3.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
